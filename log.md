# virusclass_nextflow logfile

## 2022-08-18
`To get BOWTIE2 process to run for every output tuple from FASTP, it was cruical to add the index as channel.value, to get the combinatorics!`

`bowtie`: The log file does not work with 2> .log... WHY?

#### Download kaiju virus db:
```bash
wget https://kaiju.binf.ku.dk/database/kaiju_db_viruses_2022-03-29.tgz
tar zxvf kaiju_db_viruses_2022-03-29.tgz 
```

The kaiju process works but produces a non file... Maybe because it is also from the stdout channel? 

Everything is nonefile!! WHY???
**SOLUTION TO ABOVE PROBLEM**
I had the argument cleanup = true in the .config file, which deletes content of work file. Since I had NOT put the publishDir mode as copy, the final results were just a symlink to the deleted work directory!. 

## 2022-08-19 
* Think I need to run kaiju against bigger database, containing both bacteria and viruses. 
`download the refseq database from kaijus server`
```bash
wget https://kaiju.binf.ku.dk/database/kaiju_db_refseq_2022-03-23.tgz
tar zxvf kaiju_db_refseq_2022-03-23.tgz
```

In `krona2table`the -u flag exists, which: *Unclassified reads are not counted for the total reads when calculating percentages for classified reads.*
Include or not include this? 

The KAIJU2TABLE module results in the error:
`unknown recognition error type: groovyjarjarantlr4.v4.runtime.LexerNoViableAltException` and complains that it does not have an input file. 

Trying to run in outside Nextflow:
```bash
kaiju2table -t /home/viller/virusclass/databases/kaiju_db/refseq/nodes.dmp -n /home/viller/virusclass/databases/kaiju_db/refseq/names.dmp -r genus -e -o kaiju_summary.tsv Dol1_S19_L001.kaiju.out
```
The above command works! Why does it not work inside nextflow main.nf?

**NOW the KAIJU2TABLE works, without me changing anything...** WEIRD!! 

* Download standard kraken database from https://benlangmead.github.io/aws-indexes/k2
```bash 
wget https://genome-idx.s3.amazonaws.com/kraken/k2_standard_20220607.tar.gz
```

Trying kraken2 command outside nextflow:
```bash
kraken2 \
    --db /home/viller/virusclass/databases/kraken/standard/ \
    --report testar.kraken2.report.tsv \
    --use-mpa-style \
    --use-names \
    --paired \
    /home/viller/virusclass/results/fastp/reads/Dol1_S19_L001_filt_R1.fastq.gz \
    /home/viller/virusclass/results/fastp/reads/Dol1_S19_L001_filt_R2.fastq.gz

# must unzip the krakendb tar first


```
Above command works! 

**Having problems with the megahit module. It will not save the file where it needs to go, so fiddeling around a little bit there. Otherwise it seems to work.**
FINALLY IT WORKS!
Had to do this: 
```bash
megahit \
    -1 ${reads[0]} \
    -2 ${reads[1]} \
    --out-dir megahit \
    --out-prefix ${sample_id}
    
    mv megahit/${sample_id}.contigs.fa ${sample_id}.contigs.fa
```

Added config_length.py to extract the length of the contigs generated by megahit. 
### **NB!** Had to put the .py script in the bin/ and put `#!/usr/bin/env python` in the python script!!!

**Will PROBABLY skip BUSCO, QUAST, prokka and prodigal.**
Will continue with CAT and gtdb and then... --> Phylogeny tree with mash!

### To install metabat2:
```bash
mamba install -c bioconda/label/cf201901 metabat2

```
Runned metabat2, with error:
```bash
Command error:
  [Error!] can't open input file Dol1_S19_L001_bins.table.tsv
  [Error!] There are no lines in the abundance depth file or fasta file!
```
**The bam file is generated like: align the raw reads to the assembled contigs** 
*"First we need to map the reads back against the assembly to get coverage information"*



Tutorial for binning here: https://www.youtube.com/watch?v=q9U0uTFRsl4&ab_channel=BioinformaticsVirtualCoordinationNetwork
*https://github.com/edgraham/BinSanity/wiki/Usage*
-- https://www.hadriengourle.com/tutorials/meta_assembly/

* Wrote python script for renaming fasta files to have headers like: contig_1, contig_2 etc...


### METABAT2 requires contigs > 2kb to be accurate. This is usually done by assembly of reads from many different samples from the same site. 

create CONTIGS2INDEX:
* map the reads back against the assembly to get coverage information 


## 2022-08-22 Monday

* Continue with bowtie2 alignment 
* samtools module
    * sam to bam
    * index and sort 
    
 
Which files should align to the new index of assembled contigs? The "unaligned" reads from the first bowtie2 step, or the files from FASTP? 
`Right now it aligned the host-read-filtered fastq files to the contigs index.`

**Need to output the folder name of the CONTIGS2INDEX process, not the files in it.** 
Error now:
```bash
Error executing process > 'BOWTIE2_ALIGN2CONTIGS (1)'

Caused by:  Process `BOWTIE2_ALIGN2CONTIGS (1)` terminated with an error exit status (255)Command executed:
  bowtie2     -x [Dol1_S19_L001, [/home/viller/virusclass/virusclassification_nextflow/work/71/a25811cb79674320e38718cd843fed/Dol1_S19_L001.1.bt2, /home/viller/virusclass/virusclassification_nextflow/work/71/a25811cb79674320e38718cd843fed/Dol1_S19_L001.2.bt2, /home/viller/virusclass/virusclassification_nextflow/work/71/a25811cb79674320e38718cd843fed/Dol1_S19_L001.3.bt2, /home/viller/virusclass/virusclassification_nextflow/work/71/a25811cb79674320e38718cd843fed/Dol1_S19_L001.4.bt2, /home/viller/virusclass/virusclassification_nextflow/work/71/a25811cb79674320e38718cd843fed/Dol1_S19_L001.rev.1.bt2, /home/viller/virusclass/virusclassification_nextflow/work/71/a25811cb79674320e38718cd843fed/Dol1_S19_L001.rev.2.bt2]]     -1 Dol1_S19_L001_unaligned.1.fastq     -2 Dol1_S19_L001_unaligned.2.fastq     -S Dol1_S19_L001_aligned.sam     2> Dol1_S19_L001.bowtie.log

Command exit status:
  255
```
The output from `CONTIGS2INDEX` **CANNOT** be a tuple! 
Change from
```
output:
    tuple val(sample_id), path('*.bt2'), emit: index
    
# to:
output:
    val(sample_id), emit: sample_id
    path('*.bt2'), emit: index
```

Trying to use the operator: **getParent** to get the folder of the contig index. 
Documentation: https://buildmedia.readthedocs.org/media/pdf/nextflow1/latest/nextflow1.pdf


changing from:
```
path('*.bt2'), emit: index
# to
path('${sample_id}'), emit: index
```
The above did not work, it must be: `path('*.bt2'), emit: index`. Therefore I need to get the parentfolder with operators like: operator: **getParent** 
in the main.nf script. 


Trying many different things related to all of the above, but it in the end it was hard to get it to work. The solution below, however, worked:
```nextflow
process TESTAR {  
    echo true

    input:
    tuple val(sample_id), path(reads)
    
    
    script:
    index = params.bowtie2_align2contigs.toString() + '/' + sample_id
    """
    echo ${index}
   
    """   
}
```

In CONTIS2INDEX:
```bash
# instead of:
output:
    val(sample_id), emit: sample_id
    val('*.bt2'), emit: index
# change to:
```

Testing bowtie2 from command line:
```bash
bowtie2 -x /home/viller/virusclass/databases/bowtie2/Dol1_S19_L001/Dol1_S19_L001 -1 /home/viller/virusclass/results/bowtie2/unaligned/Dol1_S19_L001_unaligned.1.fastq -2 /home/viller/virusclass/results/bowtie2/unaligned/Dol1_S19_L001_unaligned.2.fastq -S TESTARBOWTIE_aligned.sam
```
The above works...

### Now the ALIGN2CONTIGS works
Had to publishdir the new index of the contigs to a absolute path and use: 
`index = params.bowtie2_contigs_index.toString() + '/' + sample_id + '/' + sample_id`
To get it to work. 

install samtools:
```bash
mamba install samtools==1.11
```

**YES!!** Finally fixed the bowtieALIGN2CONTIGS problem (that it could not find the index). Fixed it with this line:
```bash
index = index_files[0].toRealPath().toString().split('\\.')[0]
```

Trying metabat2 command:
```bash
  metabat2 \
    -i /home/viller/virusclass/results/megahit/Dol1_S19_L001.contigs.fa \
    /home/viller/virusclass/results/bowtie2/align2contigs/aligned/Dol1_S19_L001.bam \
    --minContig 1500 \
    -o binTEST 
```
Two short contigs (<1500) in the above command, resulted in no bins.

#### Downloading new test data
```bash
curl -O -J -L https://osf.io/h9x6e/download
```

Trying workflow with new testdata:
```bash
Error executing process > 'METABAT2 (1)'

Caused by:
  Missing output file(s) `*_bins*` expected by process `METABAT2 (1)`

Command executed:

  metabat2     -i ERR321578.contigs.fa     ERR321574.bam     -o ERR321578_bins

Command exit status:
  0

Command output:
  MetaBAT 2 (v2.12.1) using minContig 2500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, and maxEdges 200. 
  0 bins (0 bases in total) formed.

Work dir:
  /home/viller/virusclass/virusclassification_nextflow/work/12/82d8fb6e82474bd5d5210d452f8713
Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`
```
Must fix so that if the output is null due to to small contigs, the output is nothing! 

```bash
 kaiju \
    -t /home/viller/virusclass/databases/kaiju_db/refseq/nodes.dmp \
    -f /home/viller/virusclass/databases/kaiju_db/refseq/kaiju_db_refseq.fmi \
    -i /home/viller/virusclass/testdata/ERR1135369_1.fastq \
    -j /home/viller/virusclass/testdata/ERR1135369_2.fastq \
    -o TESTAR.kaiju.out
```
`Error: Read names are not identical between the two input files. Probably reads are not in the same order in both files.`

### Sorting fastq files
Hmm... This error replicates although I use the **raw** files.
Is there some way to sort the fastq files???
Trying the package `pip install pyfastx`

**Seems like the reads does NOT pair up between the two files**
Trying new data:
```bash
wget http://downloads.hmpdacc.org/data/Illumina/anterior_nares/SRS018585.tar.bz2
```

Solved the problems that metabat2 sometimes does not output anything:
output: `optional: true`

## 2022-08-23

Installing metaspades:
```bash
mamba install -c bioconda spades
```

testing metaspades:
```bash
metaspades.py \
    -1 /home/viller/virusclass/testdata/test_1.fastq \
    -2 /home/viller/virusclass/testdata/test_2.fastq \
    -o test_spades 
```
Spades was much longer to run compared to megahit, and resulted in much longer contigs


Install Maxbin2:
```bash
mamba install -c bioconda maxbin2
```

**Changing the main.nf to include metaspades instead of megahit and running metabat2 on that instead**
It seems like metabat doesnt produce anything, although the contigs are longer from metaspades than from megahit. 

Will try with maxbin2. 
**I do not get it to work** 

* Run kaiju on the contig files 

### 2022-08-24

Runned the whole pipeline on large test data, and it WORKED, but was slow:
```bash
Completed at: 24-Aug-2022 00:59:33
Duration    : 8h 15m 23s
CPU hours   : 16.6
Succeeded   : 21
```

Need to increase the CPU, threads and RAM. 

* Add CAT/BAT today as well. 
* ADd human index for bowtie2

**Maybe change the metabat2 parameters** 
From 1500 to... 2500? 

#### Chaning the nextflow.config file to add cpus and so on:
* https://carpentries-incubator.github.io/workflows-nextflow/08-configuration/index.html

## This followning structure for nextflow.config works: 
```bash
params {
    fastqin = '~/virusclass/testdata/test_{1,2}.fastq'
}


process {
    
    withName: FASTP {
        cpus = 20
        memory = 10.GB
        publishDir = [
            path: { "FASTP_TEST/reads/" },
            mode: 'copy',
            pattern: "*.fastq"
        ]
    }
    withName: TEST {
        cpus = 20
        memory = 10.GB
        publishDir = [
            path: { "FASTP_TEST/reads/" },
            mode: 'copy',
            pattern: "*.txt"
        ]
        publishDir = [
            path: { "FASTP_TEST/log/" },
            mode: 'copy',
            pattern: "*.log"
        ]
    }
  
}

```

**Installing CAT/BAT**
```bash
mamba install -c bioconda cat
```

Moving the CAT and bowtie2 human index to databases in this project. Takes a lot of time unpacking CAT... 
**DONE**. 


Trying to run BAT on single MAG:
```bash
CAT bin -b /home/viller/virusclass/results/metabat2/APX_bins.1.fa -d /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_CAT_database/ -t /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy/ 
```
Worked, but slow, need to add -n for cores

running CAT add names:
```bash
CAT add_names \
    -i /home/viller/virusclass/results/out.BAT.bin2classification.txt \
    -o CAT_with_names.txt \
    -t /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy/ \
    --only_official
    
# summarise
CAT summarise \ 
    -i CAT_with_names.txt \
    -o CAT_summary.txt
```

#### about metabat2
Is is always the top >node in the fasta file that constitutes the longest bin of all nodes in the fasta file? 

Trying metabat2 again on metaspades assemblies and see what the output are, REMEMBER CORES!
```bash
metabat2 \
    -i /home/viller/virusclass/results/metaspades/APX_contigs.fasta \
    /home/viller/virusclass/results/bowtie2/align2contigs/aligned/APX.bam \
    -m 2500 \
    -t 25 \
    -o TESTING_METABAT2_bins 
```

## Stuff to remove and change:
* Remove taxonomic classification of raw reads -> just use assemblies
    * MUCH faster without! around 50% reduce of time. 
* Remove everything that has to do with metaspades, because it too slow? 

**CAT uses the max number of cores when nothing i specified**

alpha and beta diversity:
https://carpentries-incubator.github.io/metagenomics/aio/index.html


* kraken biom for diversity between samples. 

trying cat on a folder of bins:
```bash
CAT bins -b /home/viller/virusclass/results/CAT_TEST -d /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_CAT_database/ -t /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy/ -s fa
```
`-s` needed to specify the suffix of the bin files (in this case .fa)
This worked.

```bash
CAT add_names \
    -i /home/viller/virusclass/results/out.BAT.bin2classification.txt \
    -o CAT_with_names.txt \
    -t /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy/ \
    --only_official
    
CAT summarise \ 
    -i CAT_with_names.txt \
    -o CAT_summary.txt
    
    # ERROR

```
CAT summarise does not work because: 
CAT summarise currently does not support classification files wherein some contigs / MAGs have multiple classifications (as contig_2 above).


### 2022-08-25
The whole pipeline works! 

Install bracken:
```bash
mamba install -c bioconda bracken
```

Testing kraken2 without --mpa-style:

```bash
kraken2 \
    --db /home/viller/virusclass/databases/kraken/standard \
    --report TESTREPORT-KRAKEN.tsv \
    --use-names \
    /home/viller/virusclass/results/megahit/APX.contigs.fa
    
# bracken
bracken \
    -d /home/viller/virusclass/databases/kraken/standard \
    -i TESTREPORT-KRAKEN.tsv \
    -o bracken_regularreport.tsv 
    
# level = Species as default 
```

For making trees based on taxid (NCBI):
```bash
mamba install ete3
```

ete3 is downloadin local database:
```bash
(virusclass) viller@cg1:~/virusclass/virusclassification_nextflow$ ete3 ncbiquery --tree
NCBI database not present yet (first time used?)
Downloading taxdump.tar.gz from NCBI FTP site (via HTTP)...
Done. Parsing...
Loading node names...
2439350 names loaded.
282313 synonyms loaded.
Loading nodes...
2439350 nodes loaded.
Linking nodes...
Tree is loaded.
Updating database: /home/viller/.etetoolkit/taxa.sqlite ...
 2439000 generating entries... 
Uploading to /home/viller/.etetoolkit/taxa.sqlite

Inserting synonyms:      280000 
Inserting taxid merges:  65000 
Inserting taxids:       1145000 
```

removing ete3
```bash
(virusclass) viller@cg1:~$ rm -rf .etetoolkit*
```

## Fick data från Andreas:
Tja,
Ursäkta att det tagit lite tid. Jag har nu fått upp ett första dataset till vår externa ftp. Det innehåller ett alphacorona samt vår interna spik (Rift Valley Fever). Data har använts i denna artikel: https://www.mdpi.com/1999-4915/14/3/556

Running the pipe on that and changed the bowtie2 index to human genome. 

* Analysis:
    * A lot of reads coming from human, even though aligned against hg38. 
    * No bins from metabat2 were produced, and thus no CAT taxonomy was made. 
    * Could see the spike from rift valley (15%)
    
From the article:
`Using this technique, a CoV sequence
(alpha-CoV, GenBank provisional accession number OK663601, 21,882 nucleotides) spanning the near complete genome was obtained from 1 of 16 samples (6.3%), the feces of a female adult M. daubentonii collected from Tollarp (Table 1).`

When I BLAST the longest contigs from the megahit file, the longest ones match 99% with bat corona. Maybe binning is wrong approach here... Maybe to little data? 

* Should add back on kaiju on raw reads as well... 
* should add metaspades again 
* put the metabat2 -m to 1500
* removing to output the scaffold from metaspades. 


So only one of the 16 bats they could detect a whole sequence of corona. 

Running the pipeline again with:
* binning on metaspades instead of megahit
* kaiju/kraken on raw reads
* metabat2 -m set to 1500

**Note on the analysis**
* The kaiju on raw reads detected much more raft valley


**Next thing to do**
Take the longest contigs (over a treshhold... 1000?) and BLAST and show the results. 

* add bracken to raw reads. 

### WHICH DATABASE TO USE ON KAIJU AND KRAKEN

Maybe remove metaspades after all...
* Removeed metaspades from the pipe


A lot of information in the kaiju.out files! Readname and so on... 
Add -v to kaiju!! 

Add to kaiju: 
```bash
kaiju-addTaxonNames -t nodes.dmp -n names.dmp -i kaiju.out -o kaiju.names.out
```

# IDEA:

###  Depending on the length and number of contigs classified as one species -> Download the reference genome and align and assemble the contigs to the reference genome of that species. 


The NCBIWWW module from biopython is WAY to slow, and does not work...Bummer. 

# 2022-08-26

**The kaiju db and the BLAST db seems not to agree with each other**


To create new conda envs: **VERY IMPORTANT WITH env** before create:
```bash
mamba env create --file environment.yml -n name
```

Hur många reads gick per **contig**?! 

```
MEGAHIT headers:

flag is a tag to represent the connectivity of a contig in the assembly graph. flag=1 means the contig is standalone, flag=2 a looped path and flag=0 for other contigs.

multi is roughly the average kmer coverage. But the figure is not precise and if you want to quantify the coverage of a contig, I would suggest you align the reads back to the contigs by short read aligners.

Basically, you could just ignore the header and put the contigs to the subsequent analysis.
```

* run CAT contigs on the contig from megahit
* Run quast on the assembly 

Run CAT on contigs:
```bash
CAT contigs \
    -c /home/viller/virusclass/results/megahit/Bat-Guano-15_S6_L001_R.contigs.fa \
    -d /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_CAT_database/ \
    -t /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy/ 
    
CAT add_names \
    -i /home/viller/virusclass/results/out.CAT.contig2classification.txt \
    -o CAT_with_names.txt \
    -t /home/viller/virusclass/databases/CAT/CAT_prepare_20210107/2021-01-07_taxonomy/ \
    --only_official
    
CAT summarise \
    -i CAT_with_names.txt \
    -o CAT_summary.txt \
    -c megahit/Bat-Guano-15_S6_L001_R.contigs.fa \
```
* Confligt with version of DIAMOND. HAd to run the above command from the "test" conda env.


Testing metaspades:
```bash
metaspades.py \
    -1 /home/viller/virusclass/results/bowtie2/unaligned/Bat-Guano-15_S6_L001_R_unaligned.1.fastq \
    -2 /home/viller/virusclass/results/bowtie2/unaligned/Bat-Guano-15_S6_L001_R_unaligned.2.fastq \
    -o test_spades 
```
Interested in the .gfa file.

install quast:
```bash
mamba install simplejson
mamba install -c bioconda quast

######################
Executing transaction: / The default QUAST package does not include:
* GRIDSS (needed for structural variants detection)
* SILVA 16S rRNA database (needed for reference genome detection in metagenomic datasets)
* BUSCO tools and databases (needed for searching BUSCO genes) -- works in Linux only!

To be able to use those, please run
    quast-download-gridss
    quast-download-silva
    quast-download-busco
```
Quast does NOT ouput any interesting data! 

### Checkv
https://bitbucket.org/berkeleylab/checkv/src/master/
CheckV can remove *proviruses* 
```bash
mamba install checkv

checkv download_database ./

checkv end_to_end \
    -d /home/viller/virusclass/virusclassification_nextflow/databases/checkv-db-v1.4 \
    virusclassification_nextflow/results/megahit/Bat-Guano-15_S6_L001_R.contigs.fa \
    test_checkv \
    -t 50
    
```
* The output from checkv is probably also redundant, it seems like... 


#### Pilon
```bash
mamba install -c bioconda pilon


### run pilon
pilon \
    --genome /home/viller/virusclass/results/megahit/Bat-Guano-15_S6_L001_R.contigs.fa \
    --bam /home/viller/virusclass/results/bowtie2/align2contigs/aligned/Bat-Guano-15_S6_L001_R.bam \
    --outdir test_pilon \
    --vcf
    
```
https://github.com/broadinstitute/pilon
**Pilon probably redundant as well, the vcf file was bland**

#### Running the main.nf with mamba env created from test_env.yaml
```yaml
channels:
  - bioconda
  - conda-forge
  - defaults
dependencies:
  - python=3.9
  - pandas
  - megahit
  - fastp
  - kaiju
  - kraken
  - samtools=1.11
  - cat 
  - bracken
  - metabat2
  - bowtie2
  - krona
  - pip
  - nextflow
  - pip:
    - altair
    - fire
```
## The whole pipeline worked with the environment created from the above env file!


Fix CAT_CONTIGS2SUMMARY.nf

Add -p and everything to kaiju output raw as well. 
Merge the CAT file and the kaiju2names contigs and the sequences 

Trying kaiju2table on the names file:
```bash
kaiju-addTaxonNames \
    -t /home/viller/virusclass/databases/kaiju_db/refseq/nodes.dmp \
    -n /home/viller/virusclass/databases/kaiju_db/refseq/names.dmp \
    -i /home/viller/virusclass/results/kaiju/Bat-Guano-15_S6_L001_R_names.out \
    -u \
    -p \
    -o TESTINGKAIJU.out
    
    
kaiju2table \
    -t /home/viller/virusclass/databases/kaiju_db/refseq/nodes.dmp \
    -n /home/viller/virusclass/databases/kaiju_db/refseq/names.dmp \
    -r genus \
    -e \
    -o TESTINKAIJU.tsv \
    /home/viller/virusclass/results/kaiju/Bat-Guano-15_S6_L001_R_names.out
    
```
The file stays the same. 

Cleaning and merging the kaiju2name and the megahit contig file:
```python

kaiju2names = pd.read_csv('../results/kaiju/megahit/Bat-Guano-15_S6_L001_R_names_megahit.out', 
                          sep='\t', 
                          header=None,
                          index_col=False,
                          names=['classfied', 'name', 
                                 'NCBI', 'match_length', 
                                 'all_matches', 'aa_matches', 
                                 'aa_sequence', 'species']).loc[:, ['name', 'match_length', 'aa_sequence', 'species', 'NCBI']]


megahit_contigs = pd.read_csv('~/virusclass/results/megahit/Bat-Guano-15_S6_L001_R.csv')
    


merged_raw = kaiju2names.merge(megahit_contigs, on='name').sort_values('length', ascending=False)




merged = (merged_raw
    .dropna()
    .assign(summed=lambda x: x.groupby('species')['length'].transform(sum) / x['length'].sum())
    .assign(species=lambda x: x['species'].str.split(';').str[:-1])
    .assign(last_level=lambda x: x['species'].str[-1])
    .assign(second_level=lambda x: x['species'].str[-2])
    .assign(third_level=lambda x: x['species'].str[-3])

    .assign(kingdom=lambda x: np.select([x['species'].str[0] != 'cellular organisms'],
                                        [x['species'].str[0]],
                                        default=x['species'].str[1]))
)

taxonomy_list = ['kingdom', 'phylum', 'class', 'order', 'family', 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

# the below works, giving the taxonomy a list back depending on number of occurancies! 

(merged_raw
 .assign(species=lambda x: x['species'].str.split(';').str[:-1])
 
 .assign(species=lambda x: np.select([x['species'].str[0] != 'cellular organisms'],
                                     [x['species']],
                                     default=x['species'].str[1:]))
 .explode('species')
 .rename(columns={'species': 'rank'})
 .dropna()
 .assign(taxonomy=lambda x: x.groupby('name')['name'].transform(lambda x: taxonomy_list[:x.count()]))
)

merged_raw.assign(species=lambda x: x['species'].str.split(';').str[:-1])['species'].iloc[0]
```

## 2020-08-29

* Trying ways to assess the contig quality:
    * map reads to the contigs using salmon?
        * Tool which uses salmon and extract useful information: transrate 
        * Will try transrate


Install transrate:
```bash

# needs ruby (First install ruby)
mamba install -c conda-forge ruby==2.6.5

# then transrate
mamba install -c bioconda transrate
```

running transrate:
```bash
OPTIONS:
  --assembly=<s>            Assembly file(s) in FASTA format, comma-separated
  --left=<s>                Left reads file(s) in FASTQ format, comma-separated
  --right=<s>               Right reads file(s) in FASTQ format, comma-separated
  --reference=<s>           Reference proteome or transcriptome file in FASTA format
  --threads=<i>             Number of threads to use (default: 8)
  --merge-assemblies=<s>    Merge best contigs from multiple assemblies into file
  --output=<s>              Directory where results are output (will be created) (default: transrate_results)
  --loglevel=<s>            Log level. One of [error, info, warn, debug] (default: info)
  --install-deps=<s>        Install any missing dependencies. One of [read, ref, all]
  --examples                Show some example commands with explanations
 
 
transrate \
    --assembly=/home/viller/virusclass/results/megahit/Bat-Guano-15_S6_L001_R.contigs.fa \
    --left=/home/viller/virusclass/results/bowtie2/unaligned/Bat-Guano-15_S6_L001_R_unaligned.1.fastq \
    --right==/home/viller/virusclass/results/bowtie2/unaligned/Bat-Guano-15_S6_L001_R_unaligned.2.fastq \
    --output=TEST_TRANSRATE
```

### Transrate does not work and is not maintained. Try something else! 

Install mosdepth:
```bash
mamba install -c bioconda mosdepth

mosdepth test_mosdepth /home/viller/virusclass/results/bowtie2/align2contigs/aligned/Bat-Guano-15_S6_L001_R.bam
```

### use samtools mpileup to extract the coverage /bp in each contig:
```bash
samtools mpileup -o filename -a {bamfile}
```
The output are tab delimited file with:
1. chromosome name
2. position (1 index)
3. nucleotide
4. coverage

Inspiration:
https://www.biostars.org/p/104063/

Add mpileup to the nextflow workflow. 


## Dockerfile and Docker image with micromamba:
Built a docker image with micromamba as:
```dockerfile
FROM mambaorg/micromamba:0.25.1
COPY --chown=$MAMBA_USER:$MAMBA_USER env.yaml /tmp/env.yaml
RUN micromamba install -y -n base -f /tmp/env.yaml && \
    micromamba clean --all --yes
```
build the docker image:
```bash
docker build --tag virushanter .
```

To run a container from the image and be able to create files:
```bash
docker run \
    -ti \
    --rm \
    -u root \
    -v ~/virusclass/virusclassification_nextflow:/tmp virushanter \
    /bin/bash
```

The pipeline works in test environment from the env.yaml file.

Will try to run the pipeline in docker with: 
```bash
nextflow run main -with-docker virushanter
```
It cannot find the right files. 
Maybe mount the whole $HOME with:
```bash
docker {
    runOptions= "-v $HOME:$HOME"
    enabled = true
}

#Think I Will have to do: 
docker {
    runOptions= "-v $HOME/:/root"
    enabled = true
}
```

Made a new docker image with root access, using `USER root`in the Dockerfile:
```bash
FROM mambaorg/micromamba:0.25.1
COPY --chown=$MAMBA_USER:$MAMBA_USER env.yaml /tmp/env.yaml
USER root 
RUN micromamba install -y -n base -f /tmp/env.yaml && \
    micromamba clean --all --yes
WORKDIR /app
```

#### Troubles to run Docker from nextflow natively.
Therefore I have another approach, namely to run the nextflow script from inside the docker container insted:
```bash
docker run -ti --rm -v $HOME:$HOME virushanter /bin/bash

#Then from inside the container, just cd into the nextflow folder and
nextflow run main.nf
```

* Which files do I **actually** need to save from the nextflow run??
    * bowtie2 log
    * fastp log 
    * CAT file
    * mpileupfile
    * megahit csv
    * kraken files
    * kaiju files
    * krona report 


### The below command works! 
Maybe need to mount many volumes to the docker container to be able to run the command:
```bash
docker run \
    -ti \
    --rm \
    -v $HOME:$HOME \
    -v $HOME/virusclass/virusclassification_nextflow:/app \
    virushanter \
    nextflow run main.nf
```
* **Clean up in the output from the nextflow workflow**
* Remove the workdir as well


**Moved the databases and the testdata to the workdir folder.**
Maybe I do not have to mount the whole home directory now.
## The below works!
```bash
docker run \
    -ti \
    --rm \
    -v $HOME/virusclass/virusclassification_nextflow:/app \
    virushanter \
    nextflow run main.nf
```
Nextflow cannot find the databses if I dont add: /app/ to all paths to the config file

###  To automate the nextflow process from the sequencer, write a custom ***python*** script to move files and move the results folder and name stuff accordingly. 

# 2022-08-30

Today I will:

* Clean up the output from the nextflow workflow

    * fastp
        * -> log
    
    * bowtie2
        * -> log from raw and contigs
        
    * kaiju
        * raw
        * wrangled
            * plots:
                * bar
                * krona chart
    
    * kraken
        * raw
        * wrangled
            * plots:
                * bar
   
   * megahit
       * raw
       * wrangled
           * plots:
               * histogram of contig length
               * Coverage of contigs
   
   * cat
       * raw
       * wrangled
       
       
   * Output tables from everything.


# For kaiju raw:
only use the krona chart

   
   
* Clean up the code inside the jupyter notebook. 
* Organize python scripts and automate plotting

# 2022-08-31

Add version to each software in the Dockerfile and add altair_saver from conda. 
 - DONE
 
### What to save in the database?

TO assemble genomes:

haploflow:
https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02426-8

RagTag:
https://github.com/malonge/RagTag

install ragtag:
```bash
mamba install -c bioconda ragtag
```

## trying to:
this process: 
https://www.youtube.com/watch?v=ZhBx1fOk4Nc

Workflow:
```bash
# bowtie2 index on reference
# align fastq PE to index
# samtools sort sorted bam
# samtoools mpileup
# ivar consensus
```

align the fastq reads to bat alphacorona virus genome with bowtie2.
- However, NO reads aligned... why?? 
Try to the raft valley virus genome:
- 4 % reads mapped to this (samtools flagstat raft.sam)

install ivar:
```bash
mamba install -c bioconda ivar
```

run ivar consensus:
```bash
samtools mpileup -aa -A -d 1000000 -q 20 raft.bam | ivar consensus -t .6 -m 2 -p consensus
```
That worked and produced a consensus file which when runned by blast matched 99% to raft valley fever virus. 


## Trying to use nucmer and mummer to align contigs to reference:
https://taylorreiter.github.io/2019-05-11-Visualizing-NUCmer-Output/
```bash
mamba intstall -c bioconda mummer
```

```bash
nucmer --mum bat_alphacoronavirus.fa 1-bat-alpha.fa -p 1-bat

delta-filter -l 1000 -q test.delta > test_filter.delta

show-coords -c -l -L 1000 -r -T test_filter.delta > test_filter_coords.txt
```
The above **DID NOT** work.

### Trying to use minimap2:
```bash
minimap2 -a bat_alphacoronavirus.fa bat-alpha-contig.fa > bat-alpha.sam

samtools mpileup -aa -A -d 1000000 -q 20 bat-alpha.sam | ivar consensus -t .6 -m 2 -p bat_consensus

```
The above did **NOT** work! 

```bash
minimap2 -ax asm5 bat_alphacoronavirus.fa bat-alpha-contig.fa > alignment_bat.sam
samtools mpileup -aa -A -d 1000000 -q 1 alignment_bat.sam | ivar consensus -t .1 -m 1 -p alignment_consensus

```
The above did **NOT** work! 




# New approach:
### Maybe the unclassified reads from especially the contigs are the MOST interesting! do NOT filter those out from the CAT file. 

Removed the `-u`parameter from the KAIJU_MEGAHIT2NAMES.nf module (to include the unclassified)

commentet out:
```python
 # .loc[lambda x: x.classification != 'no taxid assigned']
 # .loc[lambda x: x['superkingdom'] != 'no support']
 # .loc[lambda x: x['phylum'] != 'no support']
```
in the wrangle_kaiju_megahit_cat.py file. 
Look at the outcome from this. 

Added back the `-u` to kaiju, because the output file was not usable without it. 


# 2022-09-01

Now the `wrangle_kaiju_megahit_cat.py` produces a merged csv file with all the contigs, even the unassigned ones. Maybe split this up into assigned and unassigned? 

## bandage to visualize the megahit contigs:
https://github.com/voutcn/megahit/wiki/Visualizing-MEGAHIT's-contig-graph


### adding checkv to the nextflow pipeline

Test to build new image from new env file. 
```bash
docker build --tag virushanter_new .
```
The above command yielded:
`Sending build context to Docker daemon 50 gb`
Why? 

Moved the Dockerfile and the env.yaml into its own folder:
```bash
mkdir docker
mv Dockerfile docker/
mv env.yaml docker/
```

DID NOT WORK BECAUSE pip and wroing channels. Updated the yaml file and try again! 

Trying:
```bash
docker run \
    -ti \
    --rm \
    -v "${PWD}":/app \
    virushanter_new \
    nextflow run main.nf
```
IT Works! 

# 2022-09-02

Starting to build the Flask application!

Maybe have to build locally (but Dockerize it), in order to test it properly.
Starting with that. 

`Moved the whole folder structure locally`

# 2022-09-19 

- Add unique sampleid folder after results/ folder to separate between runs. E.g. results/sample_id/bowtie2/
- Add clean: true to nextflow.config file to the finalzied version. 
- Add max threads to each process 
- Add a log file with date and nextflow output when the workflow is complete 


Changed the docker run command from: 
```bash

docker run \
    -ti \
    --rm \
    -v $HOME:$HOME \
    -v $HOME/virusclass/virusclassification_nextflow:/app \
    virushanter_new \
    nextflow run main.nf
    
#to

docker run \
    -ti \
    --rm \
    -v "${PWD}":/app \
    virushanter_new \
    nextflow run main.nf

    
```
Because it could not find the main.nf file.
The above WORKS. 

* Remove the outputdirs in the nextflow.config file and add the publishdir path directly to each module. E.g:

```bash
# instead of 
publishDir("${params.fastp_out}", pattern: "*.html", mode: 'copy')

# do
publishDir("results/${sample_id}/fastp", pattern: "*.html", mode: 'copy')
```

* Changed every module to include the sampleID now. 

- Add workflow.onComplete to nextflow.config:
```bash
workflow.onComplete = {
    // any workflow property can be used here
    println "Pipeline complete"
    println "Command line: $workflow.commandLine"
}
```

- Adding coverage plot script to the bin folder. 
- Adding altair and altair_saver to docker file env.yaml

rebuilding docker image with altair and altair_saver:
```bash
docker build --tag virushanter_altair .
```

Running the workflow with the new plot module and the new image:
```bash
docker run \
    -ti \
    --rm \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf
```

Everything works! 


# 2022-12-05

Started to run on the real data set from the MiSeq.

Some samples cannot be runned through BRACKEN_MEGAHIT (it cannot find the input file, although it is there.)
* No reads found in the input file...

* Removing BRACKEN_MEGAHIT for now to see if the pipeline runs

#### remove the METABAT2 as well? Maybe it is not needed. 
Seems like I dont use that file either way.

#### maybe add optional: true to the output?


### Fix the number of CPU and memory for the whole pipeline
When I runned the pipeline with multiple samples, it could not allocate the right amount of mem and cpus and crashed.
https://github.com/nf-core/methylseq/issues/89

Perhaps this could be useful to add to the config file:
```bash
executor.$local.cpus = 8
executor.$local.memory = '32 GB'
```

Adding the above to the nextflow.config set the ceiling so the pipeline could run with multiple samples!

#### Removed METABAT2 process from the main.nf

# 2022-12-09

### Changing `wrangle_bracken.py` to NOT drop "new_est_reads" column.

### Changed `wrangle_kaiju_raw` to:
```python
    kaiju_table = (pd.read_csv(table_file,
                              sep='\t',
                              usecols=[1, 2, 3, 4]) # add column 2: the number of reads
```
Included column 2 which contains number of reads

## The above worked and included the total number of reads!


# 230220
## Running data from nano-flowcell 
```bash
docker run \
    -ti \
    --rm \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf
```


### Got the following error:

####  Because:
* because CAT could not classify any contigs

```bash
Error executing process > 'WRANGLE_KAIJU_MEGAHIT_CAT (1)'

Caused by:
  Process `WRANGLE_KAIJU_MEGAHIT_CAT (1)` terminated with an error exit status (1)

Command executed:

  wrangle_kaiju_megahit_cat.py     sample_01_S8_names_megahit.out     CAT_sample_01_S8_contigs_names.txt     sample_01_S8.csv     sample_01_S8_cat_kaiju_merged

Command exit status:
  1

Command output:
  (empty)

Command error:
  Traceback (most recent call last):
    File "/app/bin/wrangle_kaiju_megahit_cat.py", line 60, in <module>
      fire.Fire(wrangle_kaiju_megahit_cat)
    File "/opt/conda/lib/python3.9/site-packages/fire/core.py", line 141, in Fire
      component_trace = _Fire(component, args, parsed_flag_args, context, name)
    File "/opt/conda/lib/python3.9/site-packages/fire/core.py", line 466, in _Fire
      component, remaining_args = _CallAndUpdateTrace(
    File "/opt/conda/lib/python3.9/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
      component = fn(*varargs, **kwargs)
    File "/app/bin/wrangle_kaiju_megahit_cat.py", line 42, in wrangle_kaiju_megahit_cat
      cat = (cat_raw
    File "/opt/conda/lib/python3.9/site-packages/pandas/core/frame.py", line 4512, in assign
      data[k] = com.apply_if_callable(v, data)
    File "/opt/conda/lib/python3.9/site-packages/pandas/core/common.py", line 358, in apply_if_callable
      return maybe_callable(obj, **kwargs)
    File "/app/bin/wrangle_kaiju_megahit_cat.py", line 51, in <lambda>
      .assign(kingdom_cat=lambda x: x['superkingdom'].str[:-6])
    File "/opt/conda/lib/python3.9/site-packages/pandas/core/generic.py", line 5575, in __getattr__
      return object.__getattribute__(self, name)
    File "/opt/conda/lib/python3.9/site-packages/pandas/core/accessor.py", line 182, in __get__
      accessor_obj = self._accessor(obj)
    File "/opt/conda/lib/python3.9/site-packages/pandas/core/strings/accessor.py", line 177, in __init__
      self._inferred_dtype = self._validate(data)
    File "/opt/conda/lib/python3.9/site-packages/pandas/core/strings/accessor.py", line 231, in _validate
      raise AttributeError("Can only use .str accessor with string values!")
  AttributeError: Can only use .str accessor with string values!
```


### Changed the wrangle_kaiju_megahit.py to:

* dropped the lines below the "DROPPED below"

```python
    cat = (cat_raw
     .rename(columns={'# contig': 'name',
                      'species': 'last_level_cat',
                      'genus': 'second_level_cat',
                      'family': 'third_level_cat'})
    # .loc[lambda x: x.classification != 'no taxid assigned']
    # .loc[lambda x: x['superkingdom'] != 'no support']
    # .loc[lambda x: x['phylum'] != 'no support']
     .drop(columns=['lineage', 'lineage scores'])
           
    # DROPPED the below for testing
    # .assign(kingdom_cat=lambda x: x['superkingdom'].str[:-6])
    # .drop(columns='superkingdom')
    )

    merged = kaiju.merge(cat, on='name', how='outer').sort_values('length', ascending=False)
```

### Or add fillna(" ") to convert NaN to str to mitigate the problem

# 230314
* /home/miseq-runs contains all the fastq files from the miseqruns
* added results from previous virusclassification to pandemic-preparedness/results_virusclassification_nextflow

## Running virusclassification on sample09-16 from the nano-flowcell runned 230306

```bash
docker run \
    -ti \
    --rm \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf -profile standard
```

Run the command with nextflow run main.nf -profile standard
* Still eats up all memory...

## Trying again
```bash
docker run \
    -ti \
    --rm \
    --cpus 20 \
    --memory 100gb \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf 
```


### added maxForks to the config file to see if that keeps it from taking all resources

# 20230315

* Added the following to the nextflow.config
```bash
executor.$local.cpus = 1
executor.$local.memory = '1 GB'
executor.$local.maxForks = 2
executor.$local.queueSize = 2
```

Did not work...
### trying to run with docker run --cpus 5 --memory 8gb
```bash
process {
    executor = "local"
    cpus = 1
    memory = "1 GB"
    maxForks = 2
    queueSize = 2
}
```
# 230327

Running virusclass on sample 15 and 16.
* Trying new command to restrict mem usage:
```bash
docker run \
    -ti \
    --rm \
    --cpus 20 \
    --memory 130gb \
    --memory-swap 130gb \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf 
```

## The above worked but took more time. Maybe It is possible to run all the samples at once now?


# 230512

Sequencing on regular flowcell v2 76x2 BP, 230510_M00568_0519_000000000-KV26F.
Annes project with hanta virus.

Most of the reads (90%) ended up in sample05.
Makes the analysis difficult. 
Start with Undetermined to see what is in there? Or take that last?

### Aim with the analysis:
* Assemble genomes for each sample
* Look for contaminants (run the virushanter pipeline)

### Startin with samples (evertything except sample05 and Undetermined):
sample01_S1_R1_001.fastq.gz
sample01_S1_R2_001.fastq.gz
sample02_S2_R1_001.fastq.gz
sample02_S2_R2_001.fastq.gz
sample03_S3_R1_001.fastq.gz
sample03_S3_R2_001.fastq.gz
sample04_S4_R1_001.fastq.gz
sample04_S4_R2_001.fastq.gz
sample06_S6_R1_001.fastq.gz
sample06_S6_R2_001.fastq.gz
sample07_S7_R1_001.fastq.gz
sample07_S7_R2_001.fastq.gz
sample08_S8_R1_001.fastq.gz
sample08_S8_R2_001.fastq.gz

### Running with the following:
```bash
docker run \
    -ti \
    --rm \
    --cpus 20 \
    --memory 130gb \
    --memory-swap 130gb \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf 
```
DONE in ~3,5 H!
* Moving the results to: /home/viller/clinical-genomics/pandemic-preparedness/results_virusclassification_nextflow/230510_M00568_0519_000000000-KV26F


# 20230515

Running on sample05 from the 230510_M00568_0519_000000000-KV26F run:
```bash
docker run \
    -ti \
    --rm \
    --cpus 20 \
    --memory 130gb \
    --memory-swap 130gb \
    -v "${PWD}":/app \
    virushanter_altair \
    nextflow run main.nf 
```